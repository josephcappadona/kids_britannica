# -*- coding: utf-8 -*-
"""encyclopedia_britannica_kids_scraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U4q_vpcgxDptcRumrp9OpmSXJC1XQxvK

encyclopedia britannica kids scraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GMWmMLwDTEmbc4-3UQVMYr2JN1N0mc4F
"""

#!pip install -q wget git+https://github.com/josephcappadona/m3u8downloader

import requests
import re
from bs4 import BeautifulSoup
from time import sleep
from string import ascii_lowercase
from pprint import pprint
import wget
import os
from m3u8downloader import M3u8Downloader
from pathlib import Path
import json
import shutil
from glob2 import glob
import logging
from multiprocessing import Process, Queue, Manager, Pool
from multiprocessing.dummy import Pool as ThreadPool
import time
import sys
from numpy import concatenate, random
from collections import defaultdict
from dataclasses import dataclass
from typing import List, Dict
import traceback
import random as rand

logging.basicConfig()
logging.disable(logging.CRITICAL)
logger = logging.getLogger('my_logger')
fh = logging.FileHandler('log.txt')
fh.setFormatter(logging.Formatter('%(asctime)s %(levelname)s %(message)s'))
logger.addHandler(fh)

RATE_LIMIT = 0.1  # 100ms

def get_sleep_time():
    return RATE_LIMIT * (1 + random.random())

def clean_html(raw_html):
    # adapted from https://stackoverflow.com/questions/9662346/python-code-to-remove-html-tags-from-a-string
    if not raw_html:
        return ''
    r = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')
    cleaned = re.sub(r, '', raw_html)
    return cleaned

def clean_text(text):
    return ' '.join(text.split())

session = requests.Session()
session.post('https://kids.britannica.com/login',
             data = {'username' : 'me@email.com',
                     'password' : 'password'})

"""## Link Scraping"""

def get_letter_page_urls(letter, page, tier, limit=None):
    """Retrieves the URLs for a single page of the index for a particular letter of a particular tier.

    Args:
        letter: A string, a single letter of the alphabet.
        page: A positive integer, the page of `letter`'s index to query.
        tier: A string, one of "kids", "students", or "scholars".
        limit: [optional] A positive integer, the maximum number of URLs to retrieve. Useful for testing.
    
    Returns:
        A list of strings representing URLs of articles.
    """
    sleep(get_sleep_time())
    page_urls = []
    page_url = f'https://kids.britannica.com/{tier}/browse/alpha/{letter}/{page}'
    page_soup = BeautifulSoup(session.get(page_url).text)
    content = page_soup.find('body').find('div', {'class': 'content'}).find('ul', {'class': 'seo-items'})
    for item in content.find_all('li'):
        href = item.find('dd').find('a').get('href')
        item_url = f'https://kids.britannica.com{href}'
        page_urls.append(item_url)
        if limit and len(page_urls) >= limit:
            break
    return page_urls

def get_letter_urls(letter, tier, limit=None):
    """Retrieves the URLs for a particular letter of a particular tier of Encyclopedia Britannica Kids.

    Args:
        letter: A string, a single letter of the alphabet.
        tier: A string, one of "kids", "students", or "scholars".
        limit: [optional] A positive integer, the maximum number of URLs to retrieve. Useful for testing.
    
    Returns:
        A list of strings representing URLs of articles.
    """
    letter_urls = []
    page = 1
    while True:
        page_urls = get_letter_page_urls(letter, page, tier, limit=limit)
        if not page_urls:
            break
        else:
            letter_urls.extend(page_urls)
            if limit and len(letter_urls) >= limit:
                break
            else:
                page += 1
    return letter_urls

def get_tier_urls(tier, limit=None, verbose=0):
    """Retrieves the URLs for a particular tier of Encyclopedia Britannica Kids.

    Args:
        tier: A string, one of "kids", "students", or "scholars".
        limit: [optional] A positive integer, the maximum number of URLs to retrieve. Useful for testing.
        verbose: [optional] An integer, the verbosity level.
    
    Returns:
        A list of strings representing URLs of articles.
    """
    if verbose >= 2:
        print(f"Getting {tier} URLs...")

    tier_urls = []
    for letter in ascii_lowercase:
        letter_urls = get_letter_urls(letter, tier, limit=limit)
        tier_urls.extend(letter_urls)
        if limit and len(tier_urls) >= limit:
            break

    if verbose >= 2:
        print(f"Scraped {max(len(tier_urls), limit)} {tier} URLs.")
    return tier_urls[:limit]

def get_all_urls(**kwargs):
    kids_urls = get_tier_urls('kids', **kwargs)
    students_urls = get_tier_urls('students', **kwargs)
    scholars_urls = get_tier_urls('scholars', **kwargs)
    all_urls = kids_urls + students_urls + scholars_urls
    return all_urls

"""kids_urls = get_tier_urls('kids')
print(len(kids_urls))
4758

students_urls = get_tier_urls('students')
print(len(students_urls))
22107

scholars_urls = get_tier_urls('scholars')
print(len(scholars_urls))
104898

all_urls = kids_urls + students_urls + scholars_urls
print(len(all_urls))
131763

## Article Parsing

### `scrape_article` Helpers
"""

def get_article_text(article_url):
    """Retrieves the text content of a particular Encyclopedia Britannica Kids article.

    Args:
        article_url: A string, the URL of the article to scrape.
    
    Returns:
        A tuple of the following items:
            * title [string]
            * text [list of tuples: (section_title, [paragraph1, paragraph2, ...])]
            * text_html [string]
    """
    sleep(get_sleep_time())
    article_html = session.get(article_url).text
    article_soup = BeautifulSoup(article_html)
    article_title = article_soup.find('body').find('header').find('h1').get_text()

    article_sections = article_soup.find('article').find_all('div', {'class': 'panel'})
    if not article_sections:
        article_sections = article_soup.find('article').find_all('section')

    article_text = []
    for section in article_sections:
        section_title_div = section.find('a', {'role': 'button'})
        if section_title_div:
            section_title = section_title_div.get_text().strip()
        else:
            section_title = ''

        section_text = []
        for paragraph in section.find_all('p'):
            text = clean_text(paragraph.get_text())
            if text:
                section_text.append(text)
        article_text.append((section_title, section_text))

    return article_title, article_text, article_html

def get_article_media(article_url):
    """Retrieves the media content of a particular Encyclopedia Britannica Kids article.

    Args:
        article_url: A string, the URL of the article to scrape.
    
    Returns:
        A tuple of the following items:
            * media [list of dictionaries] - each entry represents the metadata for an 
                image or video of the article and has the following keys:
                    * title [string]
                    * media-type [string] - either "IMAGE", "AUDIO", or "VIDEO"
                    * caption [string]
                    * data [dictionary]
                
                If "media-type" is "IMAGE" or "AUDIO", the "data" dictionary will have the following keys:
                    * src [string] - a URL to the source image
                
                If "media-type" is "VIDEO", the "data" dictionary will have the following keys:
                    * transcript [string]
                    * video-id [string]
                    * manifest-url [string]
                    
            * media_html [string]
    """
    sleep(get_sleep_time())
    media_url = f"{article_url}/media"
    media_html = session.get(media_url).text
    media_soup = BeautifulSoup(media_html)

    article_media = []
    media_list = media_soup.find('div', {'id': 'article-media-content'}).find('ul')
    if media_list:
        for media in media_list.find_all('li'):
            media = media.find('a')
            media_title = clean_html(media.get('data-title'))
            media_type = media.get('data-media-type')
            media_caption = clean_html(media.get('data-caption'))

            if media_type:
                media_info = {
                    'title': media_title,
                    'media-type': media_type,
                    'caption': media_caption,
                    'data': {}
                }
                if media_info['media-type'] == 'VIDEO':
                    media_info['data']['transcript'] = media.get('data-transcript') or ""
                    media_info['data']['video-id'] = video_id = media.get('data-jwplayer-id')
                    media_info['data']['manifest-url'] = f"https://content.jwplatform.com/manifests/{video_id}.m3u8"
                    manifest_url = Path(media_info['data']['manifest-url'])
                    media_info['id'] = manifest_url.stem
                    media_info['data']['file-type'] = 'mp4'

                elif media_info['media-type'] == 'IMAGE':
                    media_info['data']['src'] = media.get('data-full-path')
                    src_url = Path(media_info['data']['src'])
                    media_info['id'] = src_url.stem
                    media_info['data']['file-type'] = src_url.suffix.strip('.').lower()

                elif media_info['media-type'] == 'AUDIO':
                    media_info['data']['src'] = media.get('data-full-path')
                    src_url = Path(media_info['data']['src'])
                    media_info['id'] = src_url.stem
                    media_info['data']['file-type'] = src_url.suffix.strip('.').lower()
                article_media.append(media_info)

    return article_media, media_html

def get_related_articles_page(article_url, page):
    """Retrieves a single page of related articles.

    Args:
        article_url: A string, the URL of the article to scrape.
        page: A positive interger, the page of related articles to scrape.
    
    Returns:
        A tuple of the following items:
            * related_articles [list of strings] - the URLs to the specified page's related articles
            * related_article_page_html [list of strings] - the HTML of the specified page's related article listing
    """
    sleep(RATE_LIMIT)
    related_page_url = f"{article_url}/related/main?page={page}"
    related_page_html = session.get(related_page_url).text
    related_page_soup = BeautifulSoup(related_page_html)

    related_page_urls = []
    related_page_list = related_page_soup.find('ul', {'class': 'results'}).find_all('li')
    for related_item in related_page_list:
        related_item_href = related_item.find('a').get('href')
        related_item_url = f"https://kids.britannica.com{related_item_href}"
        related_page_urls.append(related_item_url)
    return related_page_urls, related_page_html

def get_related_articles(article_url):
    """Retrieves the related articles of a particular Encyclopedia Britannica Kids article.

    Args:
        article_url: A string, the URL of the article to scrape.
    
    Returns:
        A tuple of the following items:
            * related_articles [list of strings] - the URLs to the related articles
            * related_article_page_htmls [list of strings] - the HTMLs corresponding to the related article listings
    """
    sleep(get_sleep_time())
    related_url = f"{article_url}/related/main?page=1"
    related_html = session.get(related_url).text
    related_soup = BeautifulSoup(related_html)

    related_article_urls = []
    related_article_page_htmls = []
    related_content = related_soup.find('ul', {'class': 'results'})
    page = 1
    while True:
        try:
            related_page_urls, related_page_query_html = get_related_articles_page(article_url, page)
            related_article_urls.extend(related_page_urls)
            related_article_page_htmls.append(related_page_query_html)
            page += 1
        except:
            break

    related_website_urls, related_website_page_html = get_related_websites(article_url)

    return related_article_urls, related_article_page_htmls

def get_related_websites(article_url):
    """Retrieves the related websites of a particular Encyclopedia Britannica Kids article.

    Returns:
        A tuple of the following items:
            * related_websites [list of strings] - the URLs to the related websites
            * related_website_page_html [string] - the HTML corresponding to the related website listing
    """
    sleep(get_sleep_time())
    related_websites_url = f"{article_url}/related/websites"
    related_websites_html = session.get(related_websites_url).text
    related_websites_soup = BeautifulSoup(related_websites_html)

    related_website_urls = []
    try:
        related_website_list = related_websites_soup.find('ul', {'class': 'results-resources'}).find_all('li')
        for related_item in related_website_list:
            related_item_url = related_item.find('a').get('href')
            related_website_urls.append(related_item_url)
    except:
        pass
    return related_website_urls, related_websites_html

def get_tier_from_url(article_url):
    split = article_url.split('/')
    base_idx = split.index('kids.britannica.com')
    tier = split[base_idx + 1]
    return tier

def get_id_from_url(article_url):
    split = article_url.split('/')
    base_idx = split.index('kids.britannica.com')
    id_ = split[base_idx + 4]
    id_cleaned = re.sub('[^0-9]', '', id_)  # remove non-numeric characters
    return id_cleaned

def print_article_summary(article):
    """Prints a summary of the specified article.

    Args:
        article: A dictionary, the article to summarize.
    """
    print(article['title'])
    print(article['url'])
    n_sections = len(article['text'])
    n_words = 0
    for section_title, section_text in article['text']:
        for paragraph in section_text:
            n_words += len(paragraph.split())
    print(f"{n_sections} section(s) containing {n_words} words")
    n_media = len(article['media'])
    n_images = len([m for m in article['media'] if m['media-type'] == 'IMAGE'])
    n_videos = len([m for m in article['media'] if m['media-type'] == 'VIDEO'])
    n_audios = len([m for m in article['media'] if m['media-type'] == 'AUDIO'])
    print(f"{n_media} piece(s) of media consisting of {n_images} image(s), {n_videos} video(s), and {n_audios} audio(s)")
    n_related_articles = len(article['related_articles'])
    n_related_websites = len(article['related_websites'])
    print(f"{n_related_articles} related article(s) and {n_related_websites} related website(s)")

"""### `scrape_article`"""

def scrape_article(article_url):
    """Scrapes the text, media, and metadata of a particular Encyclopedia Britannica Kids article.

    Args:
        article_url: A string, the URL of the article to scrape.
    
    Returns:
        A dictionary with the following keys:
            * url [string]
            * id [string]
            * title [string]
            * text [list of tuples: (section_title, [paragraph1, paragraph2, ...])]
            * media [list of dictionaries]
            * related_articles [list of strings]
            * related_websites [list of strings]
            * htmls [dictionary]
    """
    artcile_id = Path(article_url).name
    tier = get_tier_from_url(article_url)
    title, text, text_html = get_article_text(article_url)
    media, media_html = get_article_media(article_url)
    related_articles, related_articles_pages_htmls = get_related_articles(article_url)
    related_websites, related_websites_page_html = get_related_websites(article_url)
    return {
        'url': article_url,
        'id': artcile_id,
        'tier': tier,
        'title': title,
        'text': text,
        'media': media,
        'related_articles': related_articles,
        'related_websites': related_websites,
        'htmls': {
            'text': text_html,
            'media': media_html,
            'related_articles': related_articles_pages_htmls,
            'related_websites': related_websites_page_html
        }
    }

"""## Writing to Disk

### `ScrapingPool` Helpers
"""

data_dir = Path('data')
articles_dir = data_dir / 'articles'
media_dir = data_dir / 'media'
kids_dir = articles_dir / 'kids'
students_dir = articles_dir / 'students'
scholars_dir = articles_dir / 'scholars'
os.makedirs(data_dir, exist_ok=True)
os.makedirs(articles_dir, exist_ok=True)
os.makedirs(media_dir, exist_ok=True)
os.makedirs(kids_dir, exist_ok=True)
os.makedirs(students_dir, exist_ok=True)
os.makedirs(scholars_dir, exist_ok=True)

def get_saved_ids(data_dir):
    data_dir = Path(data_dir)
    json_paths = glob(str(data_dir / 'articles' / '*' / '*.json'))
    ids = {Path(json_path).stem.split(' ')[0] for json_path in json_paths}
    return ids

def write_json(output_path, data):
    with open(output_path, 'w+') as f:
        json.dump(data, f)

def get_chunks(lst, n):
    """Yield successive n-sized chunks from lst."""
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

"""### `ScrapingPool`"""

def sanitize_filename(filename):
    return filename.replace('/', '_').replace(':', '-').replace('?', '').replace('*', '').replace('"', '\'')

class ArticleDownloader:
    def __init__(self, data_dir='data', pool_size=2, chunk_size=1000, max_retries=1):
        self.data_dir = Path(data_dir)
        self.pool = ThreadPool(pool_size)
        self.chunk_size = chunk_size
        self.max_retries = max_retries
    
    def get_worker_args(self, chunks):
        return zip(range(len(chunks)), chunks, [self.data_dir]*len(chunks))
    
    def download_urls(self, urls):
        # initial attempt
        print(f"Downloading {len(urls)} articles...")
        chunks = list(get_chunks(urls, self.chunk_size))
        args = self.get_worker_args(chunks)
        failed = concatenate(self.pool.map(self.batch, args))

        # retry if necessary
        retries = 0
        while len(failed) > 0 and retries < self.max_retries:
            logging.info(f"{len(failed)} failed, retrying")
            chunks = list(get_chunks(failed, self.chunk_size))
            args = self.get_worker_args(chunks)
            failed = concatenate(self.pool.map(self.batch, args))
            retries += 1
    
    @staticmethod
    def batch(data):
        id_, article_urls, data_dir = data
        
        failed = []
        for article_url in set(article_urls):
            article_id = get_id_from_url(article_url)
            try:
                article = scrape_article(article_url)
                filename = f"{article['id']} {article['title']}.json"
                filename = sanitize_filename(filename)
                filepath = data_dir / 'articles' / article['tier'] / filename
                write_json(filepath, article)
                print(f'{article_id} downloaded')
            except Exception as e:
                print(article_url)
                traceback.print_exception(type(e), e, e.__traceback__)
                failed.append(article_url)
        return failed

"""## Loading from Disk"""

class KidsBritannicaDataSet:
    def __init__(self, data_dir='data'):
        self.data_dir = Path(data_dir)
    
    @property
    def articles(self):
        for article in self.kids_articles:
            yield article
        for article in self.students_articles:
            yield article
        for article in self.scholars_articles:
            yield article
    
    @property
    def kids_articles(self):
        if not hasattr(self, 'kids_article_paths'):
            self.kids_article_paths = KidsBritannicaDataSet.get_article_paths(self.data_dir, tier='kids')
        for json_path in self.kids_article_paths:
            yield json.load(open(json_path, 'rt'))
    
    @property
    def students_articles(self):
        if not hasattr(self, 'students_article_paths'):
            self.students_article_paths = KidsBritannicaDataSet.get_article_paths(self.data_dir, tier='students')
        for json_path in self.students_article_paths:
            yield json.load(open(json_path, 'rt'))
    
    @property
    def scholars_articles(self):
        if not hasattr(self, 'scholars_article_paths'):
            self.scholars_article_paths = KidsBritannicaDataSet.get_article_paths(self.data_dir, tier='scholars')
        for json_path in self.scholars_article_paths:
            yield json.load(open(json_path, 'rt'))

    def compile_metadata(self):
        print('Compiling metadata...')
        metadata = []
        metadata_keys = ['id', 'url', 'tier', 'title']
        for article in self.articles:
            article_metadata = {key:article[key] for key in metadata_keys}
            metadata.append(article_metadata)
        return metadata

    def download_urls(self, overwrite=True, limit=None, verbose=1):
        data_dir = self.data_dir
        urls_filepath = Path(data_dir) / 'urls.json'
        if not urls_filepath.exists() or overwrite:
            all_urls = get_all_urls(limit=limit, verbose=verbose)

            write_json(urls_filepath, all_urls)
            return all_urls
    
    def download_articles(self, pctg=1.00, **kwargs):
        data_dir = self.data_dir
        url_json_path = Path(data_dir) / 'urls.json'
        if not url_json_path.exists():
            print("No URLs downloaded yet. Downloading now...")
            urls = KidsBritannicaDataSet.download_urls(data_dir=data_dir)
        else:
            print("Loading URLs from file...")
            urls = json.load(open(url_json_path, 'rt'))

        print("Loading saved article IDs...")
        saved_ids = get_saved_ids(data_dir)
        urls = [url for url in urls if get_id_from_url(url) not in saved_ids] # filter
        urls = rand.choices(urls, k=int(len(urls)*pctg)) # random selection
        print(f"{len(saved_ids)} articles already downloaded.")
        print(f"Downloading {len(urls)} articles...")

        dl = ArticleDownloader(data_dir=data_dir, **kwargs)
        dl.download_urls(urls)
    
    @staticmethod
    def download_media(data_dir='data'):
        raise NotImplemented

    @staticmethod
    def get_article_paths(data_dir, tier='*'):
        article_paths = glob(str(data_dir / 'articles' / tier / '*.json'))
        if len(article_paths) == 0:
            raise ValueError(f"No articles could not be found in {str(data_dir)}. Please download the data first.")
        return article_paths

"""### Downloading Media"""

def download_image(url, output_path):
    wget.download(url, out=str(output_path))

def download_video(url, output_path):
    url_path = Path(url)
    if url_path.suffix == '.m3u8':
        downloader = M3u8Downloader(url, output_path)
        downloader.start()
    else:
        raise ValueError()

def download_audio(url, output_path):
    wget.download(url, out=str(output_path))

"""### Running the Download"""

#ds = (data_dir='data')
#ds.download_articles()
#for article in ds.articles:
#    print(article['id'])
